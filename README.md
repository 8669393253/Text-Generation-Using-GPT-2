# Text-Generation-Using-GPT-2
This project demonstrates how to generate text based on a custom prompt using a pre-trained GPT-2 model. The code utilizes the transformers library by Hugging Face to load the GPT-2 model, encode a prompt, and generate coherent text based on that prompt.
